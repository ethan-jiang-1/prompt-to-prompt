{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Copyright 2022 Google LLC. Double-click for license information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Prompt-to-Prompt with Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ptp_global import init_model, NUM_DIFFUSION_STEPS, GUIDANCE_SCALE, LOW_RESOURCE\n",
    "ldm_stable = init_model()\n",
    "\n",
    "\n",
    "import torch\n",
    "import ptp_utils\n",
    "import ptp_view_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Prompt-to-Prompt Attnetion Controllers\n",
    "Our main logic is implemented in the `forward` call in an `AttentionControl` object.\n",
    "The forward is called in each attention layer of the diffusion model and it can modify the input attnetion weights `attn`.\n",
    "\n",
    "`is_cross`, `place_in_unet in (\"down\", \"mid\", \"up\")`, `AttentionControl.cur_step` help us track the exact attention layer and timestamp during the diffusion iference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ptp_attention import EmptyControl\n",
    "import ptp_view_utils\n",
    "\n",
    "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None):\n",
    "    if run_baseline:\n",
    "        print(\"w.o. prompt-to-prompt\")\n",
    "        images, latent = run_and_display(prompts, EmptyControl(), latent=latent, run_baseline=False, generator=generator)\n",
    "        print(\"with prompt-to-prompt\")\n",
    "    images, x_t = ptp_utils.text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=NUM_DIFFUSION_STEPS, guidance_scale=GUIDANCE_SCALE, generator=generator, low_resource=LOW_RESOURCE)\n",
    "    ptp_view_utils.view_images(images)\n",
    "    return images, x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Cross-Attention Visualization\n",
    "First let's generate an image and visualize the cross-attention maps for each word in the prompt.\n",
    "Notice, we normalize each map to 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ptp_attention import AttentionStore\n",
    "from ptp_view_utils import show_cross_attention\n",
    "\n",
    "g_cpu = torch.Generator().manual_seed(8888)\n",
    "\n",
    "prompts = [\"soup\"]\n",
    "controller = AttentionStore()\n",
    "image, x_t = run_and_display(prompts, controller, latent=None, run_baseline=False, generator=g_cpu)\n",
    "show_cross_attention(prompts, controller, res=16, from_where=(\"up\", \"down\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Re-Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptp_attention import AttentionReweight, AttentionRefine\n",
    "from ptp_attention import get_equalizer\n",
    "from ptp_attention import LocalBlend\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are my croutons?\n",
    "It might be useful to use Attention Re-Weighting with a previous edit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"soup\",\n",
    "           \"pea soup with croutons\"] \n",
    "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
    "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                             self_replace_steps=.4, local_blend=lb)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with more attetnion to `\"croutons\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"soup\",\n",
    "           \"pea soup with croutons\"] \n",
    "\n",
    "\n",
    "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
    "controller_a = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, \n",
    "                               self_replace_steps=.4, local_blend=lb)\n",
    "\n",
    "### pay 3 times more attention to the word \"croutons\"\n",
    "equalizer = get_equalizer(prompts[1], (\"croutons\",), (3,))\n",
    "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
    "                               self_replace_steps=.4, equalizer=equalizer, local_blend=lb,\n",
    "                               controller=controller_a)\n",
    "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"end\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
